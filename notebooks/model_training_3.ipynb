{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "596e6354-0adb-450b-84e7-5cd4b1921bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model, layers\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os \n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48019a83-7dd8-4091-afbf-24bfbea56ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO = tf.data.experimental.AUTOTUNE # used in tf.data.Dataset API\n",
    "\n",
    "TRAINING_FILENAMES = '../datasets/gazetrack_tfrec/train.tfrec' \n",
    "VALID_FILENAMES = '../datasets/gazetrack_tfrec/val.tfrec'\n",
    "TEST_FILENAMES = '../datasets/gazetrack_tfrec/test.tfrec' \n",
    "BATCH_SIZE = 256\n",
    "\n",
    "SEED = tf.Variable(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cff6804-40d5-4dfb-a491-497242435c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tfrecord_fn(example):\n",
    "    feature_description = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"path\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"device\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"screen_h\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"screen_w\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"face_valid\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"face_x\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"face_y\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"face_w\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"face_h\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"leye_x\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"leye_y\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"leye_w\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"leye_h\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"reye_x\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"reye_y\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"reye_w\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"reye_h\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"dot_xcam\": tf.io.FixedLenFeature([], tf.float32),\n",
    "        \"dot_y_cam\": tf.io.FixedLenFeature([], tf.float32),\n",
    "        \"dot_x_pix\": tf.io.FixedLenFeature([], tf.float32),\n",
    "        \"dot_y_pix\": tf.io.FixedLenFeature([], tf.float32),\n",
    "        \"reye_x1\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"reye_y1\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"reye_x2\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"reye_y2\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"leye_x1\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"leye_y1\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"leye_x2\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"leye_y2\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, feature_description)\n",
    "    example[\"image\"] = tf.io.decode_jpeg(example[\"image\"], channels=3)\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84317348-fd97-47d0-b52d-d57c5ad02242",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Defining Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d59bde2d-9fad-4903-b678-366e8177c76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation(image, training = True):\n",
    "    if training:\n",
    "        aug = tf.keras.Sequential([\n",
    "                layers.Resizing(128+10, 128+10),\n",
    "                layers.RandomCrop(128, 128, 256),\n",
    "                layers.Rescaling(1./255),\n",
    "                layers.Normalization(mean = (0.3741, 0.4076, 0.5425), variance = (0.0004, 0.0004, 0.0004))\n",
    "                ])\n",
    "        \n",
    "    else:\n",
    "        aug = tf.keras.Sequential([\n",
    "                layers.Resizing(128+10, 128+10),\n",
    "                layers.Rescaling(1./255),\n",
    "                layers.Normalization(mean = (0.3741, 0.4076, 0.5425), variance = (0.0004, 0.0004, 0.0004))\n",
    "                ])\n",
    "    \n",
    "    image = aug(image)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493f5490-b8bd-417e-bf60-bdaa6c723236",
   "metadata": {},
   "source": [
    "## Preprocessing on TFrec extracted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a51fdf6b-6417-4d1c-a578-5545d4febc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sample(features):\n",
    "    image = features['image']\n",
    "    w = tf.shape(image)[0]\n",
    "    h = tf.shape(image)[1]\n",
    "    \n",
    "    w = tf.cast(w, tf.int64)\n",
    "    h = tf.cast(h, tf.int64)\n",
    "    \n",
    "    screen_w, screen_h = features['screen_w'], features['screen_h']\n",
    "    \n",
    "    kps = [features['leye_x1']/w, features['leye_y1']/h, features['leye_x2']/w, features['leye_y2']/h,\n",
    "           features['reye_x1']/w, features['reye_y1']/h, features['reye_x2']/w, features['reye_y2']/h]\n",
    "    # kps has type float64\n",
    "    \n",
    "\n",
    "    lx, ly, lw, lh = features['leye_x'], features['leye_y'], features['leye_w'], features['leye_h']\n",
    "    rx, ry, rw, rh = features['reye_x'], features['reye_y'], features['reye_w'], features['reye_h']\n",
    "    \n",
    "    # lx, ly, lw, lh = tf.cast((lx, ly, lw, lh), tf.int32)\n",
    "    # rx, ry, rw, rh = tf.cast((rx, ry, rw, rh), tf.int32)\n",
    "    \n",
    "    lx = tf.cast(lx, tf.int32)\n",
    "    ly = tf.cast(ly, tf.int32)\n",
    "    lw = tf.cast(lw, tf.int32)\n",
    "    lh = tf.cast(lh, tf.int32)\n",
    "    \n",
    "    rx = tf.cast(rx, tf.int32)\n",
    "    ry = tf.cast(ry, tf.int32)\n",
    "    rw = tf.cast(rw, tf.int32)\n",
    "    rh = tf.cast(rh, tf.int32)\n",
    "\n",
    "    # l_eye = tf.image.crop_to_bounding_box(image, tf.math.maximum(0, ly), tf.math.maximum(0, lx), ly+lh, lx+lw)\n",
    "    # r_eye = tf.image.crop_to_bounding_box(image, tf.math.maximum(0, ry), tf.math.maximum(0, rx), ry+rh, rx+rw)\n",
    "    \n",
    "    l_eye = tf.image.crop_to_bounding_box(image, tf.math.maximum(0, ly), tf.math.maximum(0, lx), tf.math.maximum(0, lh), tf.math.maximum(0, lw))\n",
    "    r_eye = tf.image.crop_to_bounding_box(image, tf.math.maximum(0, ry), tf.math.maximum(0, rx), tf.math.maximum(0, rh), tf.math.maximum(0, rw))\n",
    "    \n",
    "\n",
    "    \n",
    "    l_eye = tf.image.flip_left_right(l_eye)\n",
    "    \n",
    "    out = [features['dot_xcam'], features['dot_y_cam']]\n",
    "    # out has type float32\n",
    "    \n",
    "    l_eye = augmentation(l_eye)\n",
    "    r_eye = augmentation(r_eye)\n",
    "    \n",
    "    # return l_eye, r_eye, kps, out, screen_w, screen_h\n",
    "    \n",
    "    # x = {'l_eye': l_eye, 'r_eye':r_eye, 'kps':kps} \n",
    "    x = (l_eye, r_eye, kps)\n",
    "    y = out\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b826a23-4abc-4ed2-a963-1f89bdcc6942",
   "metadata": {},
   "source": [
    "## Extracting the TFRecs and preprocessing+transforms+batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f0f9bf4c-c999-4e64-b692-7a8a491aaea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batched_dataset(filenames, batch_size):\n",
    "    option_no_order = tf.data.Options()\n",
    "    option_no_order.deterministic = False  # disable order, increase speed\n",
    "    \n",
    "    dataset = (\n",
    "        tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n",
    "        .with_options(option_no_order)\n",
    "        .map(parse_tfrecord_fn, num_parallel_calls=AUTO)\n",
    "        .map(prepare_sample, num_parallel_calls=AUTO)\n",
    "        .shuffle(batch_size*10)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(buffer_size=AUTO)\n",
    "    )\n",
    "    \n",
    "    dataset_len = sum(1 for _ in tf.data.TFRecordDataset(filenames))\n",
    "    print(f\"No. of samples: {dataset_len}\")\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a85b6f45-9a28-4630-9acf-790fd3667baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of samples: 43458\n"
     ]
    }
   ],
   "source": [
    "# train_dataset = get_batched_dataset(TRAINING_FILENAMES, BATCH_SIZE)\n",
    "valid_dataset = get_batched_dataset(VALID_FILENAMES, BATCH_SIZE)\n",
    "# test_dataset = get_batched_dataset(TEST_FILENAMES, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574d35d9-1238-40a7-8293-e558e17d55ee",
   "metadata": {},
   "source": [
    "## dataset shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fc31a4c3-f305-4e90-bf07-b480d7b52759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(iter(valid_dataset))\n",
    "len(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0b57891b-5ac6-469b-9563-0bc37170a40a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tuple, 3)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sample[0]\n",
    "type(x), len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "be79ced8-867b-4629-a83f-09479212086f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 128, 128, 3) (256, 128, 128, 3) (256, 8)\n"
     ]
    }
   ],
   "source": [
    "# l_eye = x['l_eye']\n",
    "# r_eye = x['r_eye']\n",
    "# lms = x['kps']\n",
    "\n",
    "l_eye, r_eye, lms = x\n",
    "\n",
    "print(l_eye.shape, r_eye.shape, lms.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0e263050-c513-4be2-83be-d27d35035bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([256, 2])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = sample[1]\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0b703e-98a6-496d-bce4-2b91b6f57a9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa05011c-3ae3-4ea9-a0b4-381b4db994ea",
   "metadata": {},
   "source": [
    "## New model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d35aa01d-93a0-4788-a838-87f1040a30a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class eye_model(layers.Layer):\n",
    "    def __init__(self, name='Eye-model'):\n",
    "        super(eye_model, self).__init__()\n",
    "\n",
    "        self.conv1 = layers.Conv2D(32, kernel_size=7, strides=2, padding='valid') \n",
    "        self.conv2 = layers.Conv2D(64, kernel_size=5, strides=2, padding='valid')\n",
    "        self.conv3 = layers.Conv2D(128, kernel_size=3, strides=1, padding='valid')\n",
    "        self.bn1 = layers.BatchNormalization(axis = -1, momentum=0.9)\n",
    "        self.bn2 = layers.BatchNormalization(axis = -1, momentum=0.9)\n",
    "        self.bn3 = layers.BatchNormalization(axis = -1, momentum=0.9)\n",
    "        self.leakyrelu = layers.LeakyReLU(alpha=0.01)\n",
    "        self.avgpool = layers.AveragePooling2D(pool_size=2)\n",
    "        self.dropout = layers.Dropout(rate=0.02)\n",
    "\n",
    "    def call(self, input_image):\n",
    "        x = self.conv1(input_image)\n",
    "        x = self.bn1(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.dropout(x) \n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def summary(self):\n",
    "        x = Input(shape=(128, 128, 3))\n",
    "        model = Model(inputs=[x], outputs=self.call(x))\n",
    "        return model.summary()\n",
    "\n",
    "\n",
    "\n",
    "class landmark_model(layers.Layer):\n",
    "    def __init__(self, name='Landmark-model'):\n",
    "        super(landmark_model, self).__init__()\n",
    "\n",
    "        self.dense1 = layers.Dense(128)\n",
    "        self.dense2 = layers.Dense(16)\n",
    "        self.dense3 = layers.Dense(16)\n",
    "        self.bn1 = layers.BatchNormalization(axis = -1,momentum=0.9)\n",
    "        self.bn2 = layers.BatchNormalization(axis = -1, momentum=0.9)\n",
    "        self.bn3 = layers.BatchNormalization(axis = -1, momentum=0.9)\n",
    "        self.relu = layers.ReLU()\n",
    "\n",
    "    def call(self, input_kps):\n",
    "        x = self.dense1(input_kps)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.dense2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.dense3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)   \n",
    "\n",
    "        return x\n",
    "    \n",
    "    def summary(self):\n",
    "        x = Input(shape=(8, ))\n",
    "        model = Model(inputs=[x], outputs=self.call(x))\n",
    "        return model.summary()\n",
    "\n",
    "class gazetrack_model(Model):\n",
    "    def __init__(self, name='Gazetrack-model'):\n",
    "        super(gazetrack_model, self).__init__()\n",
    "\n",
    "        self.eye_model = eye_model()\n",
    "        self.landmark_model = landmark_model()\n",
    "\n",
    "        self.dense1 = layers.Dense(8)\n",
    "        self.dense2 = layers.Dense(4)\n",
    "        self.dense3 = layers.Dense(2)\n",
    "\n",
    "        self.bn1 = layers.BatchNormalization(axis = -1, momentum=0.9)\n",
    "        self.bn2 = layers.BatchNormalization(axis = -1, momentum=0.9)\n",
    "        self.dropout = layers.Dropout(rate=0.12)\n",
    "        self.relu = layers.ReLU()\n",
    "\n",
    "\n",
    "    def call(self, l_r_lms):\n",
    "        # leftEye = l_r_lms['l_eye']\n",
    "        # rightEye = l_r_lms['r_eye']\n",
    "        # lms = l_r_lms['kps']\n",
    "        \n",
    "        leftEye, rightEye, lms = l_r_lms\n",
    "        \n",
    "        l_eye_feat = self.eye_model(leftEye)\n",
    "        r_eye_feat = self.eye_model(rightEye)\n",
    "        \n",
    "        l_eye_feat = layers.Flatten()(l_eye_feat)\n",
    "        r_eye_feat = layers.Flatten()(r_eye_feat)\n",
    "\n",
    "    \n",
    "        lm_feat = self.landmark_model(lms)\n",
    "        \n",
    "        combined_feat = tf.concat((l_eye_feat, r_eye_feat, lm_feat),1)\n",
    "\n",
    "        x = self.dense1(combined_feat)\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dense3(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def summary(self):\n",
    "        input1 = Input(shape=(128,128,3))\n",
    "        input2 = Input(shape=(128,128,3))\n",
    "        input3 = Input(shape=(8, ))\n",
    "\n",
    "        model = Model(inputs=[input1, input2, input3], outputs=self.call([input1, input2, input3]))\n",
    "        return model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2c14aa54-4e1a-44e3-ade4-78d81ec1f68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_19 (InputLayer)          [(None, 128, 128, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 61, 61, 32)   4736        ['input_19[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_68 (BatchN  (None, 61, 61, 32)  128         ['conv2d_30[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_10 (LeakyReLU)     multiple             0           ['batch_normalization_68[0][0]', \n",
      "                                                                  'batch_normalization_69[0][0]', \n",
      "                                                                  'batch_normalization_70[0][0]'] \n",
      "                                                                                                  \n",
      " average_pooling2d_10 (AverageP  multiple            0           ['leaky_re_lu_10[0][0]',         \n",
      " ooling2D)                                                        'leaky_re_lu_10[1][0]',         \n",
      "                                                                  'leaky_re_lu_10[2][0]']         \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           multiple             0           ['average_pooling2d_10[0][0]',   \n",
      "                                                                  'average_pooling2d_10[1][0]',   \n",
      "                                                                  'average_pooling2d_10[2][0]']   \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 13, 13, 64)   51264       ['dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_69 (BatchN  (None, 13, 13, 64)  256         ['conv2d_31[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 4, 4, 128)    73856       ['dropout_14[1][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_70 (BatchN  (None, 4, 4, 128)   512         ['conv2d_32[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 130,752\n",
      "Trainable params: 130,304\n",
      "Non-trainable params: 448\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "eye_mod = eye_model()\n",
    "eye_mod.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0306d665-e6b8-4d93-af7a-d02f9a5ff791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_20 (InputLayer)          [(None, 8)]          0           []                               \n",
      "                                                                                                  \n",
      " dense_42 (Dense)               (None, 128)          1152        ['input_20[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_71 (BatchN  (None, 128)         512         ['dense_42[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_14 (ReLU)                multiple             0           ['batch_normalization_71[0][0]', \n",
      "                                                                  'batch_normalization_72[0][0]', \n",
      "                                                                  'batch_normalization_73[0][0]'] \n",
      "                                                                                                  \n",
      " dense_43 (Dense)               (None, 16)           2064        ['re_lu_14[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_72 (BatchN  (None, 16)          64          ['dense_43[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_44 (Dense)               (None, 16)           272         ['re_lu_14[1][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_73 (BatchN  (None, 16)          64          ['dense_44[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,128\n",
      "Trainable params: 3,808\n",
      "Non-trainable params: 320\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lm_mod = landmark_model()\n",
    "lm_mod.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d840a6da-ca44-42c8-8347-bbbbcfea06a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_21 (InputLayer)          [(None, 128, 128, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " input_22 (InputLayer)          [(None, 128, 128, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " eye_model_11 (eye_model)       (None, 2, 2, 128)    130752      ['input_21[0][0]',               \n",
      "                                                                  'input_22[0][0]']               \n",
      "                                                                                                  \n",
      " input_23 (InputLayer)          [(None, 8)]          0           []                               \n",
      "                                                                                                  \n",
      " flatten_4 (Flatten)            (None, 512)          0           ['eye_model_11[0][0]']           \n",
      "                                                                                                  \n",
      " flatten_5 (Flatten)            (None, 512)          0           ['eye_model_11[1][0]']           \n",
      "                                                                                                  \n",
      " landmark_model_11 (landmark_mo  (None, 16)          4128        ['input_23[0][0]']               \n",
      " del)                                                                                             \n",
      "                                                                                                  \n",
      " tf.concat_1 (TFOpLambda)       (None, 1040)         0           ['flatten_4[0][0]',              \n",
      "                                                                  'flatten_5[0][0]',              \n",
      "                                                                  'landmark_model_11[0][0]']      \n",
      "                                                                                                  \n",
      " dense_48 (Dense)               (None, 8)            8328        ['tf.concat_1[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_80 (BatchN  (None, 8)           32          ['dense_48[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 8)            0           ['batch_normalization_80[0][0]'] \n",
      "                                                                                                  \n",
      " re_lu_16 (ReLU)                multiple             0           ['dropout_16[0][0]',             \n",
      "                                                                  'batch_normalization_81[0][0]'] \n",
      "                                                                                                  \n",
      " dense_49 (Dense)               (None, 4)            36          ['re_lu_16[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_81 (BatchN  (None, 4)           16          ['dense_49[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_50 (Dense)               (None, 2)            10          ['re_lu_16[1][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 143,302\n",
      "Trainable params: 142,510\n",
      "Non-trainable params: 792\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "main_mod = gazetrack_model()\n",
    "main_mod.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26182de6-2f87-414c-9816-1b2b7c252607",
   "metadata": {},
   "source": [
    "## Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "52cd3d8d-942d-4a1f-8cd7-52151d5fd19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.016\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "metrics = [tf.keras.metrics.mean_squared_error]\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr,beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', verbose=1, mode='min')\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "05ea77fd-479e-4462-8370-655a560e7496",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gazetrack_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2d9a2f2e-c3c1-47ba-abfc-bd63520a98b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fd3d9b-56b0-4707-b122-643cbeaaa342",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7da4910a-31db-4932-b6ff-430251e87d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18/Unknown - 107s 5s/step - loss: 18.1288 - mean_squared_error: 18.1288"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nassertion failed: [width must be >= target + offset.]\n\t [[{{node crop_to_bounding_box/Assert_4/Assert}}]]\n\t [[IteratorGetNext]] [Op:__inference_train_function_141248]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-a7c0f65dde93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0;31m#auto=1, 1=progress bar, 2=one line per epoch( maybe use 2 if running job)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/localscratch/s0mnaths.31462833.0/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/localscratch/s0mnaths.31462833.0/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nassertion failed: [width must be >= target + offset.]\n\t [[{{node crop_to_bounding_box/Assert_4/Assert}}]]\n\t [[IteratorGetNext]] [Op:__inference_train_function_141248]"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    x=valid_dataset,   \n",
    "    batch_size=batch_size,\n",
    "    epochs=1,  \n",
    "    verbose='auto',   #auto=1, 1=progress bar, 2=one line per epoch( maybe use 2 if running job)\n",
    "    callbacks=[scheduler],\n",
    "    validation_data=valid_dataset,\n",
    "    shuffle=True,    #probably will not work as our dataset is a tf.data object\n",
    "    initial_epoch=0,     #epoch at which to resume training\n",
    "    workers=1,\n",
    "    use_multiprocessing=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10409aaf-92bc-4d04-89ae-05b01a1e98c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(\n",
    "    x=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    verbose='auto',\n",
    "    steps=1,\n",
    "    callbacks=None,\n",
    "    max_queue_size=10,\n",
    "    workers=1,\n",
    "    use_multiprocessing=False,\n",
    "    return_dict=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
