{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4acb9fa7-c568-4bf0-8ce5-929b71463a64",
   "metadata": {},
   "source": [
    "## Import goodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eadf4e8d-b37e-437a-8a6a-6bdc11ccfe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os \n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7577479c-e644-4696-995a-f79ba82f8478",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO = tf.data.experimental.AUTOTUNE # used in tf.data.Dataset API\n",
    "\n",
    "TRAINING_FILENAMES = '../datasets/gazetrack_tfrec/train.tfrec'\n",
    "VALID_FILENAMES = '../datasets/gazetrack_tfrec/val.tfrec'\n",
    "TEST_FILENAMES = '../datasets/gazetrack_tfrec/test.tfrec'\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "SEED = tf.Variable(256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6838faf4-4ce6-4bd5-9ca9-4ae075207cc1",
   "metadata": {},
   "source": [
    "## TFRec extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbcba9ae-2678-4bea-9fdd-820d4a18cdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tfrecord_fn(example):\n",
    "    feature_description = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"path\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"device\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"screen_h\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"screen_w\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"face_valid\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"face_x\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"face_y\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"face_w\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"face_h\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"leye_x\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"leye_y\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"leye_w\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"leye_h\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"reye_x\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"reye_y\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"reye_w\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"reye_h\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"dot_xcam\": tf.io.FixedLenFeature([], tf.float32),\n",
    "        \"dot_y_cam\": tf.io.FixedLenFeature([], tf.float32),\n",
    "        \"dot_x_pix\": tf.io.FixedLenFeature([], tf.float32),\n",
    "        \"dot_y_pix\": tf.io.FixedLenFeature([], tf.float32),\n",
    "        \"reye_x1\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"reye_y1\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"reye_x2\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"reye_y2\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"leye_x1\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"leye_y1\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"leye_x2\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"leye_y2\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, feature_description)\n",
    "    example[\"image\"] = tf.io.decode_jpeg(example[\"image\"], channels=3)\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b00f11e-eea1-4053-b5ff-382f4506e05c",
   "metadata": {},
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18f013df-ecbb-4623-a0c5-9692e6b462af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation(image, training = True):\n",
    "    if training:\n",
    "        aug = tf.keras.Sequential([\n",
    "                layers.Resizing(128+10, 128+10),\n",
    "                layers.RandomCrop(128, 128, 256),\n",
    "                layers.Rescaling(1./255),\n",
    "                layers.Normalization(mean = (0.3741, 0.4076, 0.5425), variance = (0.0004, 0.0004, 0.0004))\n",
    "                ])\n",
    "        \n",
    "    else:\n",
    "        aug = tf.keras.Sequential([\n",
    "                layers.Resizing(128+10, 128+10),\n",
    "                layers.Rescaling(1./255),\n",
    "                layers.Normalization(mean = (0.3741, 0.4076, 0.5425), variance = (0.0004, 0.0004, 0.0004))\n",
    "                ])\n",
    "    \n",
    "    image = aug(image)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcdaf1f-d350-49a4-a1b6-eaa6c28743f0",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9858953-5838-4bba-b6ca-71221a67955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sample(features):\n",
    "    image = features['image']\n",
    "    w = tf.shape(image)[0]\n",
    "    h = tf.shape(image)[1]\n",
    "    \n",
    "    w = tf.cast(w, tf.int64)\n",
    "    h = tf.cast(h, tf.int64)\n",
    "    \n",
    "    screen_w, screen_h = features['screen_w'], features['screen_h']\n",
    "    \n",
    "    kps = [features['leye_x1']/w, features['leye_y1']/h, features['leye_x2']/w, features['leye_y2']/h,\n",
    "           features['reye_x1']/w, features['reye_y1']/h, features['reye_x2']/w, features['reye_y2']/h]\n",
    "    # kps has type float64\n",
    "\n",
    "    lx, ly, lw, lh = features['leye_x'], features['leye_y'], features['leye_w'], features['leye_h']\n",
    "    rx, ry, rw, rh = features['reye_x'], features['reye_y'], features['reye_w'], features['reye_h']\n",
    "    \n",
    "    lx = tf.cast(lx, tf.int32)\n",
    "    ly = tf.cast(ly, tf.int32)\n",
    "    lw = tf.cast(lw, tf.int32)\n",
    "    lh = tf.cast(lh, tf.int32)\n",
    "    \n",
    "    rx = tf.cast(rx, tf.int32)\n",
    "    ry = tf.cast(ry, tf.int32)\n",
    "    rw = tf.cast(rw, tf.int32)\n",
    "    rh = tf.cast(rh, tf.int32)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # l_eye = tf.image.crop_to_bounding_box(image, max(0, ly), max(0, lx), max(0, lh), max(0, lw))  \n",
    "    # r_eye = tf.image.crop_to_bounding_box(image, max(0, ry), max(0, rx), max(0, rh), max(0, rw))\n",
    "    \n",
    "    l_eye = tf.image.crop_to_bounding_box(image, ly, lx, lh, lw)  \n",
    "    r_eye = tf.image.crop_to_bounding_box(image, ry, rx, rh, rw)\n",
    "    \n",
    "    l_eye = tf.image.flip_left_right(l_eye)\n",
    "    \n",
    "    out = [features['dot_xcam'], features['dot_y_cam']]\n",
    "    # out has type float32\n",
    "    \n",
    "    l_eye = augmentation(l_eye)\n",
    "    r_eye = augmentation(r_eye)\n",
    "    \n",
    "    x = {'l_eye': l_eye, 'r_eye':r_eye, 'kps':kps}\n",
    "    y = out\n",
    "    \n",
    "    return x, y\n",
    "    # return l_eye, r_eye, kps, out, screen_w, screen_h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0fc594-489b-47ea-a687-2d7305ce8ab9",
   "metadata": {},
   "source": [
    "## Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d59e8ac7-7ae7-4be3-bcdb-04ea033e98b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def get_batched_dataset(filenames, batch_size):\n",
    "    option_no_order = tf.data.Options()\n",
    "    option_no_order.deterministic = False  # disable order, increase speed\n",
    "    \n",
    "    dataset = (\n",
    "        tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n",
    "        .with_options(option_no_order)\n",
    "        .map(parse_tfrecord_fn, num_parallel_calls=AUTO)\n",
    "        .map(prepare_sample, num_parallel_calls=AUTO)\n",
    "        .shuffle(batch_size*10)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(buffer_size=AUTO)\n",
    "    )\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf02266f-09d2-4be1-a45a-e84d56adbd59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of train samples: 398654\n",
      "No. of val samples: 43458\n",
      "No. of test samples: 59563\n"
     ]
    }
   ],
   "source": [
    "train_dataset = get_batched_dataset(TRAINING_FILENAMES, BATCH_SIZE)\n",
    "valid_dataset = get_batched_dataset(VALID_FILENAMES, BATCH_SIZE)\n",
    "test_dataset = get_batched_dataset(TEST_FILENAMES, BATCH_SIZE)\n",
    "\n",
    "train_len = sum(1 for _ in tf.data.TFRecordDataset(TRAINING_FILENAMES))\n",
    "val_len = sum(1 for _ in tf.data.TFRecordDataset(VALID_FILENAMES))\n",
    "test_len = sum(1 for _ in tf.data.TFRecordDataset(TEST_FILENAMES))\n",
    "\n",
    "print(f\"No. of train samples: {train_len}\")\n",
    "print(f\"No. of val samples: {val_len}\")\n",
    "print(f\"No. of test samples: {test_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7534ed2c-f185-438d-8f1d-4e2a48f55173",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8aff164-822f-4658-974d-c0bfa591cdf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "256\n",
      "256\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for features in train_dataset.take(1):\n",
    "    print(len(features))\n",
    "    print(len(features[0]))\n",
    "    print(len(features[0]['l_eye']))\n",
    "    print(len(features[1]))\n",
    "    print(len(features[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdbb5e68-55f4-4446-8bbf-c25dfc189e81",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-82262c257ba1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# print(features.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "for features in train_dataset.take(1):\n",
    "    # print(features.shape)\n",
    "    print(features[0].shape)\n",
    "    print(features[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f357b9-ffec-4e55-9a15-f0d919816a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "for features in loaded_dataset.take(1): #reading first 1 values from the dataset\n",
    "    for key in features.keys():\n",
    "        if key != \"image\":\n",
    "            print(f\"{key}: {features[key]}\")\n",
    "\n",
    "    print(f\"Image shape: {features['image'].shape}\")\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.imshow(features[\"image\"].numpy())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40f4025-c377-47e8-b7de-b4eecc1c442b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6818ecb-e63c-4d8c-bfad-5205e4a4c983",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
