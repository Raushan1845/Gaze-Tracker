{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9221262-43d1-4a22-8972-99392fcae8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f86a1da0-751b-4d29-a6d9-044d43ec0563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os \n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cae86a0-3e3b-43c5-abe0-7ea1357672a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO = tf.data.experimental.AUTOTUNE # used in tf.data.Dataset API\n",
    "\n",
    "TRAINING_FILENAMES = '../datasets/gazetrack_tfrec/train.tfrec'\n",
    "VALID_FILENAMES = '../datasets/gazetrack_tfrec/val.tfrec'\n",
    "TEST_FILENAMES = '../datasets/gazetrack_tfrec/test.tfrec'\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "SEED = tf.Variable(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d51df13-4744-49d6-bc29-c629faea0ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tfrecord_fn(example):\n",
    "    feature_description = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"path\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"device\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"screen_h\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"screen_w\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"face_valid\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"face_x\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"face_y\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"face_w\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"face_h\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"leye_x\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"leye_y\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"leye_w\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"leye_h\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"reye_x\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"reye_y\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"reye_w\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"reye_h\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"dot_xcam\": tf.io.FixedLenFeature([], tf.float32),\n",
    "        \"dot_y_cam\": tf.io.FixedLenFeature([], tf.float32),\n",
    "        \"dot_x_pix\": tf.io.FixedLenFeature([], tf.float32),\n",
    "        \"dot_y_pix\": tf.io.FixedLenFeature([], tf.float32),\n",
    "        \"reye_x1\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"reye_y1\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"reye_x2\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"reye_y2\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"leye_x1\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"leye_y1\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"leye_x2\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"leye_y2\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, feature_description)\n",
    "    example[\"image\"] = tf.io.decode_jpeg(example[\"image\"], channels=3)\n",
    "    return example\n",
    "\n",
    "\n",
    "def augmentation(image, training = True):\n",
    "    if training:\n",
    "        aug = tf.keras.Sequential([\n",
    "                layers.Resizing(128+10, 128+10),\n",
    "                layers.RandomCrop(128, 128, 256),\n",
    "                layers.Rescaling(1./255),\n",
    "                layers.Normalization(mean = (0.3741, 0.4076, 0.5425), variance = (0.0004, 0.0004, 0.0004))\n",
    "                ])\n",
    "        \n",
    "    else:\n",
    "        aug = tf.keras.Sequential([\n",
    "                layers.Resizing(128+10, 128+10),\n",
    "                layers.Rescaling(1./255),\n",
    "                layers.Normalization(mean = (0.3741, 0.4076, 0.5425), variance = (0.0004, 0.0004, 0.0004))\n",
    "                ])\n",
    "    \n",
    "    image = aug(image)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def prepare_sample(features):\n",
    "    image = features['image']\n",
    "    w = tf.shape(image)[0]\n",
    "    h = tf.shape(image)[1]\n",
    "    \n",
    "    w = tf.cast(w, tf.int64)\n",
    "    h = tf.cast(h, tf.int64)\n",
    "    \n",
    "    screen_w, screen_h = features['screen_w'], features['screen_h']\n",
    "    \n",
    "    kps = [features['leye_x1']/w, features['leye_y1']/h, features['leye_x2']/w, features['leye_y2']/h,\n",
    "           features['reye_x1']/w, features['reye_y1']/h, features['reye_x2']/w, features['reye_y2']/h]\n",
    "    # kps has type float64\n",
    "    \n",
    "\n",
    "    lx, ly, lw, lh = features['leye_x'], features['leye_y'], features['leye_w'], features['leye_h']\n",
    "    rx, ry, rw, rh = features['reye_x'], features['reye_y'], features['reye_w'], features['reye_h']\n",
    "    \n",
    "    # lx, ly, lw, lh = tf.cast((lx, ly, lw, lh), tf.int32)\n",
    "    # rx, ry, rw, rh = tf.cast((rx, ry, rw, rh), tf.int32)\n",
    "    \n",
    "    lx = tf.cast(lx, tf.int32)\n",
    "    ly = tf.cast(ly, tf.int32)\n",
    "    lw = tf.cast(lw, tf.int32)\n",
    "    lh = tf.cast(lh, tf.int32)\n",
    "    \n",
    "    rx = tf.cast(rx, tf.int32)\n",
    "    ry = tf.cast(ry, tf.int32)\n",
    "    rw = tf.cast(rw, tf.int32)\n",
    "    rh = tf.cast(rh, tf.int32)\n",
    "    \n",
    "    # l_eye = tf.image.crop_to_bounding_box(image, max(0, ly), max(0, lx), max(0, lh), max(0, lw))  \n",
    "    # r_eye = tf.image.crop_to_bounding_box(image, max(0, ry), max(0, rx), max(0, rh), max(0, rw))\n",
    "    \n",
    "    l_eye = tf.image.crop_to_bounding_box(image, ly, lx, lh, lw)  \n",
    "    r_eye = tf.image.crop_to_bounding_box(image, ry, rx, rh, rw)\n",
    "    \n",
    "    l_eye = tf.image.flip_left_right(l_eye)\n",
    "    \n",
    "    out = [features['dot_xcam'], features['dot_y_cam']]\n",
    "    # out has type float32\n",
    "    \n",
    "    l_eye = augmentation(l_eye)\n",
    "    r_eye = augmentation(r_eye)\n",
    "    \n",
    "    # model_in = (l_eye, r_eye, kps)\n",
    "    # return model_in, out\n",
    "    return l_eye, r_eye, kps, out\n",
    "    # return l_eye, r_eye, kps, out, screen_w, screen_h\n",
    "\n",
    "def get_batched_dataset(filenames, batch_size):\n",
    "    option_no_order = tf.data.Options()\n",
    "    option_no_order.deterministic = False  # disable order, increase speed\n",
    "    \n",
    "    dataset = (\n",
    "        tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n",
    "        .with_options(option_no_order)\n",
    "        .map(parse_tfrecord_fn, num_parallel_calls=AUTO)\n",
    "        .map(prepare_sample, num_parallel_calls=AUTO)\n",
    "        .shuffle(batch_size*10)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(buffer_size=AUTO)\n",
    "    )\n",
    "    \n",
    "    dataset_len = sum(1 for _ in tf.data.TFRecordDataset(filenames))\n",
    "    print(f\"No. of train samples: {dataset_len}\")\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac96e3ca-1ac2-4644-92ce-52b52aa8675d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of train samples: 43458\n"
     ]
    }
   ],
   "source": [
    "# train_dataset = get_batched_dataset(TRAINING_FILENAMES, BATCH_SIZE)\n",
    "valid_dataset = get_batched_dataset(VALID_FILENAMES, BATCH_SIZE)\n",
    "# test_dataset = get_batched_dataset(TEST_FILENAMES, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85cead5c-a01e-42fb-8c23-14495b0368e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterator = iter(train_dataset)\n",
    "# iterator.get_next()\n",
    "sample = next(iter(valid_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0b73b57-ac28-46d0-b0f1-b9e73039cc98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([128, 128, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limg, rimg, lms, out = sample\n",
    "\n",
    "limg[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d923f20e-aa8e-4fd1-b5af-6b9fa10eb63c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8,), dtype=float64, numpy=\n",
       "array([0.43125   , 0.79791667, 0.5203125 , 0.78958333, 0.2265625 ,\n",
       "       0.78125   , 0.309375  , 0.79375   ])>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lms[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c3a5200-3ee0-4699-b82e-6b66393a8838",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array = tf.expand_dims(limg[0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c387d6e-4b76-49b1-867d-b973c2b0b1ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 128, 128, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "887bb1ff-4128-4275-ac1e-ee9839b4a2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for features in train_dataset.take(1):\n",
    "#     print(len(features[1]))\n",
    "#     print(len(features[0]))\n",
    "#     print(len(features[0][0]))\n",
    "    \n",
    "    \n",
    "    \n",
    "#     image = features['image']\n",
    "    \n",
    "    \n",
    "#     print(f\"lefteye shape: {l_eye.shape}\")\n",
    "#     print(f\"righteye shape: {l_eye.shape}\")\n",
    "#     plt.figure(figsize=(7, 7))\n",
    "#     plt.imshow(l_eye)\n",
    "#     plt.show()\n",
    "    \n",
    "#     plt.figure(figsize=(7, 7))\n",
    "#     plt.imshow(r_eye)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ee388b-14e9-4ed9-8aaf-7b67340b4dac",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64d5f64f-717b-40c6-8c23-94d552730437",
   "metadata": {},
   "outputs": [],
   "source": [
    "class eye_model(Model):\n",
    "  def __init__(self):\n",
    "    super(eye_model, self).__init__(name='')\n",
    "\n",
    "    self.conv1 = layers.Conv2D(32, kernel_size=7, strides=2, padding='valid', name='em-conv2d1')\n",
    "    self.conv2 = layers.Conv2D(64, kernel_size=5, strides=2, padding='valid', name='em-conv2d2')\n",
    "    self.conv3 = layers.Conv2D(128, kernel_size=3, strides=1, padding='valid', name='em-conv2d3')\n",
    "    self.bn = layers.BatchNormalization(axis = 1, momentum=0.9, name='em-bn')\n",
    "    self.leakyrelu = layers.LeakyReLU(alpha=0.01, name='em-leaky-relu') \n",
    "    self.avgpool = layers.AveragePooling2D(pool_size=2, name='em-relu')\n",
    "    self.dropout = layers.Dropout(rate=0.02, name='em-dropout')\n",
    "    \n",
    "\n",
    "  def call(self, input_tensor):\n",
    "    x = self.conv1(input_tensor)\n",
    "    x = self.bn(x)\n",
    "    x = self.leakyrelu(x)\n",
    "    x = self.avgpool(x)\n",
    "    x = self.dropout(x)\n",
    "    \n",
    "    x = self.conv2(x)\n",
    "    x = self.bn(x)\n",
    "    x = self.leakyrelu(x)\n",
    "    x = self.avgpool(x)\n",
    "    x = self.dropout(x)\n",
    "    \n",
    "    x = self.conv3(x)\n",
    "    x = self.bn(x)\n",
    "    x = self.leakyrelu(x)\n",
    "    x = self.avgpool(x)\n",
    "    x = self.dropout(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "class landmark_model(Model):\n",
    "  def __init__(self):\n",
    "    super(landmark_model, self).__init__(name='')\n",
    "\n",
    "    self.dense1 = layers.Dense(128, name='lm-dense1')\n",
    "    self.dense2 = layers.Dense(16, name='lm-dense2')\n",
    "    self.dense3 = layers.Dense(16, name='lm-dense3')\n",
    "    self.bn = layers.BatchNormalization(momentum=0.9, name='lm-bn')\n",
    "    self.relu = layers.ReLU(name='lm-relu')\n",
    "\n",
    "  def call(self, input_tensor):\n",
    "    x = self.dense1(input_tensor)\n",
    "    x = self.bn(x)\n",
    "    x = self.relu(x)\n",
    "    \n",
    "    x = self.dense2(x)\n",
    "    x = self.bn(x)\n",
    "    x = self.relu(x)\n",
    "    \n",
    "    x = self.dense3(x)\n",
    "    x = self.bn(x)\n",
    "    x = self.relu(x)   \n",
    "    \n",
    "    return x\n",
    "\n",
    "class gazetrack_model(Model):\n",
    "  def __init__(self):\n",
    "    super(gazetrack_model, self).__init__(name='')\n",
    "\n",
    "    self.eye_model = eye_model()\n",
    "    self.lmModel = landmark_model()\n",
    "    \n",
    "    self.dense1 = layers.Dense(8, name='gm-dense1')\n",
    "    self.dense2 = layers.Dense(4, name='gm-dense2')\n",
    "    self.dense3 = layers.Dense(2, name='gm-dense3')\n",
    "    \n",
    "    self.bn = layers.BatchNormalization(momentum=0.9, name='gm-bn')\n",
    "    self.dropout = layers.Dropout(rate=0.12, name='gm-dropout')\n",
    "    self.relu = layers.ReLU(name='gm-relu')\n",
    "\n",
    "    \n",
    "\n",
    "  def call(self, model_in):\n",
    "    leftEye, rightEye, lms = model_in\n",
    "    l_eye_feat = tf.reshape(self.eye_model(leftEye), (3, 128*128))\n",
    "    r_eye_feat = tf.reshape(self.eye_model(rightEye), (3, 128*128))\n",
    "    \n",
    "    lm_feat = self.lmModel(lms)\n",
    "    \n",
    "    combined_feat = tf.concat((l_eye_feat, r_eye_feat, lm_feat),1)\n",
    "    \n",
    "    x = self.dense1(combined_feat)\n",
    "    x = self.bn(x)\n",
    "    x = self.dropout(x)\n",
    "    x = self.relu(x)\n",
    "    \n",
    "    x = self.dense2(x)\n",
    "    x = self.bn(x)\n",
    "    x = self.relu(x)\n",
    "    \n",
    "    x = self.dense3(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f666ce3a-ab14-4a12-a41f-beac4129ec63",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.016\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr,beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', verbose=1, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d5e7d79-c2b6-4c77-9ba6-4c9826eaeb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20ee66e5-3cae-46b2-8eb9-dcfa5ea537aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# strategy = tf.distribute.MirroredStrategy()\n",
    "# print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "# with strategy.scope():\n",
    "\n",
    "model = gazetrack_model()\n",
    "\n",
    "# # input_shape = (256, 32, 32, 3)\n",
    "# model.build((leftEye, rightEye, lms))\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb16557-db80-417e-9c9b-2dd2d034d946",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e561ffa-769f-4155-bb4e-a6af97e00263",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss=loss, metrics=[tf.keras.metrics.mean_squared_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f352428b-3e8a-41cf-a608-8f6b28f0fc6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-6ba8cfe9698c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m model.fit(\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# model_predict = Model.predict()\n",
    "# model_predict_batch = Model.predict_on_batch()\n",
    "\n",
    "model.fit(\n",
    "    x=train_dataset,   \n",
    "    batch_size=batch_size,  \n",
    "    epochs=1,  \n",
    "    verbose='auto',   #auto=1, 1=progress bar, 2=one line per epoch( maybe use 2 if running job)\n",
    "    callbacks=[scheduler],\n",
    "    validation_data=valid_dataset,\n",
    "    shuffle=True,    #probably will not work as our dataset is a tf.data object\n",
    "    initial_epoch=0,     #epoch at which to resume training\n",
    "    workers=1,\n",
    "    use_multiprocessing=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523f5976-c741-4ff8-b5fa-66504ac1a3be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede67075-34dd-43bc-9aa5-2e300e53529a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE\n",
    "model.save(args.model_output + '/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be134d74-ef8b-4b4e-af66-79e0e6df0dd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397a883c-3274-4aaa-8be1-61a9e425e445",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3465ba9-33d0-4051-addc-3fa6e71f9cb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdfa8be-27d7-4f17-b75a-d93c5e054e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dde05e-a04c-48ef-a4de-54af9d8a4f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRA\n",
    "validation_array = np.array(list(validation_dataset.unbatch().take(-1).as_numpy_iterator()))\n",
    "test_x = np.stack(validation_array[:,0])\n",
    "test_y = np.stack(validation_array[:,1])\n",
    "\n",
    "# Use the model to predict the labels\n",
    "test_predictions = model.predict(test_x)\n",
    "test_y_pred = np.argmax(test_predictions, axis=1)\n",
    "test_y_true = np.argmax(test_y, axis=1)\n",
    "\n",
    "# Evaluating model accuracy and logging it as a scalar for TensorBoard hyperparameter visualization.\n",
    "accuracy = sklearn.metrics.accuracy_score(test_y_true, test_y_pred)\n",
    "tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)\n",
    "logging.info('Test accuracy:{}'.format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
